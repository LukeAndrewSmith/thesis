
This section describes references

\section{Pose Estimation}
The existing pipeline for 2D pose estimation is based on Open Pose \cite{openPose}.


\section{Motion Priors}

The authors of HuMoR \cite{humor} presented a novel approach for learning and using a plausible motion prior. They train a conditional VAE that learns a distribution over latent transitions, in a canonical reference frame, between $\textit{states}$ that consist of a root translation, 3D joint positions, joint angles, and the respective velocities. They most notably use this model as a prior in a 'test time optimisation', which generates plausible sequence motions optimising for an initial state and a sequence of transitions starting from frame by frame estimates (2D/3D joints or points clouds). This optimisation includes, alongside others, a motion prior term based upon the conditional distribution $p(z_t|x_{t-1})$ that encourages plausible motion for the learned sequence. Note that the CVAE decoder also predicts ground plan contact alongside change in state, which are used as regularisers during their main use case 'test time optimisation'. The test time optimisation can operate on many modalities, 2D/3D joints, point clouds, etc., as the optimisation contains a Data Term $\epsilon_{data}$ that can be tailored to the modality as the HuMoR state is information rich, containing 3D joints (hence can fit to 2D joints through projection or directly to 3D) and can parametrise the SMPL model (hence the SMPL mesh can be correlated to point clouds).

I would see the success of HuMoR in occlusion situations to be largly due to the SMPL prior.


HuMoR discussions:
\begin{itemize}
    \item They consider extending the method to include body shape parameters in the state an important direction for improved generalisation.
    \item They claim normalising flows and neural ODEs show potential but they only link to papers explaining these concepts and not actually using them for this purpose so not sure (Normalising flow: map to a simple distribution with an invertible function => tractable marginal likelihood (unlike with VAEs where we have to deal with an ELBO), but I'm not sure we care about the marginal likelihood in this case)
    \item They claim 'MVAE' does not work well
    \item The SMPL regularisation and the learned contional prior are important during training
    \item Assumptions:
    \begin{itemize}
        \item The method necessitates knowledge of the ground plane, which is presently needed (empirical observation) for convergence during training (as the dataset is of motions with a flat ground), and thus also at test time even though it is not conceptually necessary
        \item Assumes static camera
    \end{itemize}
    \item Limitations:
    \begin{itemize}
        \item Single person formulation
    \end{itemize}
\end{itemize}


The authors of HuMoR \cite{humor} were inspired by the Motion VAE \cite{TODO} paper. This paper uses an Conditional VAE (with assumed standard normal prior conditioning (vs. NN in HuMoR)) that directly outputs the next state (rather than the change in state in HuMoR). The model is used Autoregressively to predict motion (rather than the main presented use of HuMoR which is to fit motion to a sequence of existing 2D/3D joint predictions, though HuMoR can equally well be used autoregressively), and is trained with the typical ELBO in a supervised manner.
Some notes to self about MotionVAE
\begin{itemize}
    \item MotionVAE is used with Deep RL with the action space taken to be the latent space of the CVAE, with a reward function that defines goals of a character, the control policy walks through the actions space to guide the generative model in accordance with these goals. Could be interesting for interactive character animation
    \item Their state representation has some differences to HuMoR, notably that the root position his projected onto the ground.
    \item Contains a nice overview of motion prediction methods
    \item Latent dimension size: 32 (typical physics based humanoid degrees of freedom)
    \item Some notes about things they mention in the related work section:
    \begin{itemize}
        \item They cite [Wang et al. 2019] who train a stochastic generative model with output $\textit{processed by a refiner network to remove foot skating and add robustness}$.
    \end{itemize}
    \item Main differences to HuMoR
    \begin{itemize}
        \item c.f discussion section in HuMoR
        \item Conditional prior
        \item Predict change in motion
        \item Predict ground contacts
        \item Much additional regularisation in training
        \item Difference state representation
        \item Use of SMPL by HuMoR
        \item Difference in network architectures
        \begin{itemize}
            \item HuMoR just uses MLPs and MVAE decoder is a 'MANN-style mixture-of-expert neural network' (6 networks, gating network weighting their outputs)
            \item RELU in HuMoR, ELU in MVAE
            \item MVAE decoder has latent variable input at each layer (not sure about HuMoR)
        \end{itemize}
        \item 
    \end{itemize}
\end{itemize}


\subsection{Overview of Approaches}
We are most interested in models that learn plausible, task independent, human motion. These are refered to by \cite{MVAE} as $\textit{Motion-then-control}$ models. We limit our scope to parametric models.
\begin{itemize}
    \item MVAE \cite{humor}
    \begin{itemize}
        \item Standard normal CVAE
        \item Outputs next pose
        \item Decoder is mixture of networks
        \item Trained with rollout and scheduled sampling
        \item State positions referenced to root projection onto ground
        \item Nice investigation into using RL in the latent space for character control
    \end{itemize}
    \item HuMoR \cite{humor}
    \begin{itemize}
        \item Parametrised conditional prior CVAE
        \item Outputs change in state and person ground contacts
        \item SMPL regularisers (a subset of their state parametrises the SMPL model)
        \item Motion learned in a canonical reference frame (TODO: not sure about MVAE)
        \item Trained without rollout (I beleive?) 
        \item State positions referenced as in SMPL model (to $(0,0)$?)
    \end{itemize}

    \item $\textbf{TODO: Things I haven't looked into so deeply}$
    \item Mixture-density network RNNs (MDN-RNNS)
    \begin{itemize}
        \item Referenced in \cite{MVAE}
        \item Output a distribution as a gaussian mixture model
    \end{itemize}
    \item Time-convolutional autoencoders
    \begin{itemize}
        \item Referenced in \cite{MVAE}
        \item Learns a latent motion manifold
        \item Followup paper also referenced
    \end{itemize}
    \item Humor claims normalising flows and neural ODEs show potential but they only link to papers explaining these concepts and not actually using them for this purpose so not sure
\end{itemize}


\subsection{General notes}
Training VAEs:
\begin{itemize}
    \item Posterior collapse
    \begin{itemize}
        \item Decoder ignores latent variable and overfits to the training sequences
        \item MVAE: input latent variable at each stage of the decoder
        \item Weighting of $\beta$-VAE
    \end{itemize}
    \item Quality vs. generalisation
    \begin{itemize}
        \item Depends on weighting of KL and Reconstructions Losses
        \item MVAE find empirically that: $\textit{MVAE reconstruction and KL losses being within one order of magnitude of eachother is a good proxy to balance}$
    \end{itemize}
    \item Stable sequence prediction
    \begin{itemize}
        \item Not sure we care so much as we probably won't be extrapolating new sequences
        \item MAVE: Scheduled sampling during training - model progressively learns to deal with it's own predictions
    \end{itemize}
\end{itemize}