\section{Motion AutoEncoders}

A well-explored model is that of the AutoEncoder (AE) \cite{bank2021_autoencoders} or Variation-AutoEncoder (VAE) \cite{kingma2022_VAE}. These are popular models as they encourage the learning of a latent representation \cite{bank2021_autoencoders} of human motion, thus the intuition is that they learn not just to reproduce the data, but actually how humans move, providing a more robust prior.

% c.f MEVA for nice related work section

Holden et al. \cite{ConvAutoEnv2015} \cite{ConvAutoEnv2016} present simple CNN-based autoencoder architectures that operate on motion sequences. The notion of skeletal aware convolutions and pooling/unpooling operations for a VAE, alongside a sliding window method for motion rectification, are presented by the authors of \cite{HierarchicalMotionVAE}. \cite{TransformerVAEPrior} presents a novel approach of leaning a latent space, then projecting directly to this latent space from a motion sequence using a separate model, again operating directly on a motion sequence. The authors of MEVA \cite{MEVA} postulate that a VAE often learns only smooth motion, as we are asking too much of the model, thus present a pipeline in which a smooth motion and coarse motion VAEs are jointly used. Starke et. al present DeepPhase \cite{DeepPhase}, an autoencoder with a latent space enforced to match sinusoidal functions that represent periodic motion. Contrary to a common trope in sequence-level models, the authors of \cite{learnedInbetweening} and of \cite{MotionVAE} operate in a frame-to-frame regime, predicting the temporally local change of motion. Finally, a number of works present the Conditional-VAE architecture \cite{CVAE} as a base with varying state representations, conditioning variables and loss terms, \cite{humor, learnedInbetweening, MotionVAE, structured4Dlatentspace}.

As we can see, the literature is rich and diverse, but we found ourselves drawn to the HuMoR model \cite{humor}, due to its state-of-the-art performance and its use for the exact task that we desire to solve, that of rectifying a motion sequence captured through frame by frame pose estimation. The authors of \cite{humor} present a C-VAE architecture that learns a distribution over latent transitions, conditioned on the previous pose. They use this architecture alongside an optimisation method that rectifies human motion obtained from, among other modalities, RGB video through frame-by-frame pose estimation.