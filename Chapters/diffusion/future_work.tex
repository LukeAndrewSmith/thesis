\section{Future Work}
\label{sec:future_work}

We have demonstrated that diffusion models represent an exciting research avenue and have layed a foundation of code for Disney Research|Studios. The future directions that could build upon this work are numerous and potentially rich in their rewards. We will discuss some of the most promising avenues for future work in this section, and will also discuss some of the shortcomings of the work presented in this thesis.

\subsection{Model}
Some of the shortcomings of the model seen in Inpainting tasks, most notably in the Inbetweening task, are due to an inconsistency between the specified known motion and the motion subsequently generated during inpainting by the model. For the inbetweening task, if we avoid overwiting the generated motion with the known motion in the last denoising step, the motion is smooth over the whole sequence and the transition between known and generated motion contains no jump. The generated and known motion however does not match perfectly even at the last denoising step, that is to say the part in the generated motion which will be overwridden by the known motion is not exactly the same as the known motion, and so when we override the generated motion with the known motion for the last time, we loose the smoothness of the motion and introduce a discontinuity between the generated and known motion. This effect might be mitigated with through a number of strategies. Firstly we might condition the model with an indication of where a known area is specified and penalise it during training for deviating from this known motion (as described in \secref{sec:autocomplete} but for sequence level motion). Another method would simply be that of a postprocessing step, in which the known motion and the generated motion could be mixed for a number of frames at the transition point, thus achieving a smooth motion between the two.

The model training could also be improved. The stopping criteria is currently taken as the point where the loss visually seems to have converged on the loss graphs. This however is not a robust method as it does not not take into account the possibility of overfitting or the possibility of decreasing performance. This can be mitigated through the use of validation metrics as described in \secref{sec:future_work_evaluation}, or by a method such as that presented in \cite{MDM}, in which they retrospectively choose the checkpoint that minimizes the FID score.

\subsection{Model use}

We noted through our studies that during the denoising procedure, the denoised $x_0$ often changed drastically throughout the process. At the beginning of the procedure when the network is predicting a denoised $x_0$ from gaussian noise, it often resorts to predicting some mean motion. As this is encorporated into the latents the predicted $x_0$ motion shows more and more diversity, what we noted however is that this motion can change drastically, and can do so late in the denoising process. Intuition would suggest that as we become closer to the end of the denoising process, where there is less and less noise present in the latent variables we are denoising, the model should become more and more certain of it's predicted motion and should not drastically change it. Howewer it sometimes did exactly that. This phenomena is worth investigation, as it may indicate a flaw in the network, or perhaps represents an area for improvement.

\subsection{Model evaluation}
\label{sec:future_work_evaluation}

We presented an entirely qualitative evaluation of the models. As discussed previously, this was motivated by a shorter available time and a desire to get a better understanding of the models before investing time in a more rigorous evaluation. This however does not result in publishable or definite work, rather a stepping stone toward such work indicating that it might be fruitful. Future work should therefore make more effort to investigate the models quantitatively. Relevant metrics should be explored for the tasks at hand (FID, KID, precision/recall, diversity \cite{Paper_with_useful_metrics_cited_by_MDM}). Alternative baseline models should be discussed as well and comparisons should be made. For example the missing state evaluation where a new subset of the joints is removed each frame might well be accurately reconstructed by simple linear interpolation between the missing frames, so the models performance might not be so impressive afterall.

            
\subsection{Data}
\TODO{Cite some paper that says data is more important that anything}
We note that our dataset, with \TODO{X} minutes of motion spanning \TODO{X} subjects is significantly smaller than the AMASS \cite{amass} dataset that contains more that 40hours of motion data spanning 300 subjects. We therefore expect that our model will generalise less well for than if it were trained on the AMASS dataset. We also note however that this is not possible due to the licensing restrictions on the AMASS dataset limiting it to non-commercial use. If any of the models we explored in this thesis or in subsequent work are to be productionized, it would be prudent to obtain a larger dataset of motion, especially considering that the use cases in which such a system would be most helpful to an animator might well be those dealing with motion far from the data distribution we currently have, as the more obscure and intricate the motion sequence the harder it would be to animate and thus more value our system could provide.